<!doctype html>
<meta charset="utf-8">
<meta name="viewport" content="width=1080">
<script src="assets/lib/template.v1.js"></script>
<script type="text/front-matter">
  title: Introduction to optimization algorithms
  description: A bird-eye view of optimization algorithms.
  authors:
    - Fabian Pedregosa: http://fa.bianp.net
  affiliations:
    - Google AI
</script>

<!-- OpenGraph Info -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
        Macros: {
          RR: "{\\mathbb{R}}",
          argmin: "{\\mathop{\\mathrm{arg\\,min}}}",
          minimize: "{\\mathop{\\mathrm{minimize}}}",
          bold: ["{\\bf #1}",1],
          xx: "{\\boldsymbol x}",
          uu: "{\\boldsymbol u}",
          yy: "{\\boldsymbol y}",
          ss: "{\\boldsymbol s}",
        }
      },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<link rel="stylesheet" type="text/css" href="assets/widgets.css">

<!-- Required -->
<script src="assets/lib/lib.js"></script>
<script src="assets/utils.js"></script>
<script>
  var renderQueue = [];
  function renderMath(elem) {
    // renderMathInElement(
    //     elem,
    //     {
    //         delimiters: [
    //             {left: "$$", right: "$$", display: true},
    //             {left: "$", right: "$", display: false},
    //         ]
    //     }
    // );
  }

  var deleteQueue = [];
  function renderLoading(figure) {
    var loadingScreen = figure.append("svg")
    .style("width", figure.style("width"))
    .style("height", figure.style("height"))
    .style("position","absolute")
    .style("top", "0px")
    .style("left","0px")
    .style("background","white")
    .style("border", "0px dashed #DDD")
    .style("opacity", 1)

    return function(callback) { loadingScreen.remove() };

  }

</script>
<div id="math-cache" style="display: none;">
  <dt-math class="star">\star</dt-math>
  <dt-math class="plus">+</dt-math>
  <dt-math class="minus">-</dt-math>
  <dt-math class="equals">=</dt-math>
  <dt-math class="alpha">\alpha</dt-math>
  <dt-math class="lambda">\lambda</dt-math>
  <dt-math class="beta">\beta</dt-math>
  <dt-math class="r">R</dt-math>
  <dt-math class="alpha-equals">\alpha=</dt-math>
  <dt-math class="beta-equals">\beta=</dt-math>
  <dt-math class="beta-equals-zero">\beta = 0</dt-math>
  <dt-math class="beta-equals-one">\beta=1</dt-math>
  <dt-math class="alpha-equals-one-over-lambda-i">\alpha = 1/\lambda_i</dt-math>
  <dt-math class="model">\text{model}</dt-math>
  <dt-math class="p">0 p_1</dt-math>
  <dt-math class="phat">0 \bar{p}_1</dt-math>
  <dt-math class="two-sqrt-beta">2\sqrt{\beta}</dt-math>
  <dt-math class="lambda-i">\lambda_i</dt-math>
  <dt-math class="lambda-i-equals-zero">\lambda_i = 0</dt-math>
  <dt-math class="alpha-gt-one-over-lambda-i">\alpha > 1/\lambda_i</dt-math>
  <dt-math class="max-sigma-one">\max\{|\sigma_1|,|\sigma_2|\} > 1</dt-math>
  <dt-math class="x-i-k">x_i^k - x_i^*</dt-math>
  <dt-math class="xi-i">\xi_i</dt-math>
  <dt-math class="beta-equals-one-minus">\beta = (1 - \sqrt{\alpha \lambda_i})^2</dt-math>
</div>
<script>
  function MathCache(id) {
    return document.querySelector("#math-cache ." + id).innerHTML;
  }
</script>
<svg style="display: none;">
  <g id="pointerThingy">
    <circle fill="none" stroke="#FF6C00" stroke-linecap="round" cx="0" cy="0" r="14"/>
    <circle fill="#FF6C00" cx="0" cy="0" r="11"/>
    <path id="XMLID_173_" fill="#FFFFFF" d="M-3.2-1.3c0-0.1,0-0.2,0-0.3c0-0.1,0-0.2,0-0.3c-0.6,0-1.2,0-1.8,0c0,0.6,0,1.2,0,1.8
      c0.2,0,0.4,0,0.6,0c0-0.4,0-0.8,0-1.2c0,0,0.1,0,0.1,0c0.3,0,0.5,0,0.8,0C-3.4-1.3-3.3-1.3-3.2-1.3c0,0.2,0,0.4,0,0.6
      c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0,0,0,0-0.1c0-1.6,0-3.2,0-4.8c0-0.6,0-1.2,0-1.8c0,0,0,0,0.1,0
      c0.3,0,0.7,0,1,0c0.1,0,0.1,0,0.2,0c0-0.2,0-0.4,0-0.6c-0.4,0-0.8,0-1.2,0C-2-7.2-2-7-2-6.8c0,0,0,0-0.1,0c-0.2,0-0.3,0-0.5,0
      c0,0,0,0-0.1,0c0,1.8,0,3.6,0,5.5c-0.2,0-0.3,0-0.4,0C-3.1-1.3-3.2-1.3-3.2-1.3z M1.1-3.7C1-3.8,1-3.8,1.1-3.7C1-4,1-4.1,1-4.3
      c0,0,0,0,0-0.1c-0.4,0-0.8,0-1.2,0c0-0.8,0-1.6,0-2.4c-0.2,0-0.4,0-0.6,0c0,1.8,0,3.6,0,5.5c0.2,0,0.4,0,0.6,0c0-0.8,0-1.6,0-2.4
      c0,0,0.1,0,0.1,0C0.3-3.7,0.6-3.7,1.1-3.7C1-3.7,1-3.7,1.1-3.7C1.1-3.7,1-3.7,1.1-3.7c0,0.8,0,1.6,0,2.3c0,0,0,0.1,0,0.1
      c0.2,0,0.4,0,0.6,0c0-0.6,0-1.2,0-1.8c0.4,0,0.8,0,1.2,0c0,0.8,0,1.6,0,2.4c0.2,0,0.4,0,0.6,0c0-0.6,0-1.2,0-1.8c0.2,0,0.4,0,0.6,0
      c0,0,0,0,0,0.1c0,0.1,0,0.3,0,0.4c0,0,0,0.1,0,0.1c0.2,0,0.4,0,0.5,0c0,0,0.1,0,0.1,0.1c0,0.2,0,0.5,0,0.7c0,1.1,0,2.3,0,3.4
      c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0c0,0,0,0,0,0c0,0.6,0,1.1,0,1.7c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0c0,0.4,0,0.8,0,1.2
      c-1.6,0-3.2,0-4.9,0c0-0.4,0-0.8,0-1.2c-0.2,0-0.4,0-0.6,0C-2,3.8-2,3.4-2,3c-0.2,0-0.4,0-0.6,0c0,0.4,0,0.8,0,1.2
      c0.2,0,0.4,0,0.6,0C-2,4.8-2,5.4-2,6c2,0,4.1,0,6.1,0c0-0.1,0-0.2,0-0.3c0-0.5,0-0.9,0-1.4c0-0.1,0-0.1,0-0.2c0.2,0,0.4,0,0.5,0
      c0.1,0,0.1,0,0.1-0.1c0-0.4,0-0.9,0-1.3c0-0.1,0-0.3,0-0.4c0.1,0,0.2,0,0.3,0c0.1,0,0.2,0,0.3,0c0-1.4,0-2.8,0-4.3
      c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c-0.4,0-0.8,0-1.2,0c0-0.2,0-0.4,0-0.6
      c-0.1,0-0.2,0-0.3,0c-0.4,0-0.9,0-1.3,0C1.2-3.7,1.1-3.7,1.1-3.7z M-3.2,1.8c0,0.4,0,0.8,0,1.2c0.2,0,0.4,0,0.5,0
      c0.1,0,0.1,0,0.1-0.1c0-0.3,0-0.6,0-1c0-0.1,0-0.1,0-0.2C-2.8,1.8-3,1.8-3.2,1.8c0-0.4,0-0.8,0-1.2c-0.2,0-0.4,0-0.6,0
      c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0,0,0,0,0.1c0,0.1,0,0.3,0,0.4c0,0.2,0,0.5,0,0.7
      c0,0,0,0.1,0.1,0.1c0.1,0,0.2,0,0.3,0C-3.4,1.8-3.3,1.8-3.2,1.8z"/>
    <path id="XMLID_172_" fill="#FFFFFF" d="M4.1,4.2C4.1,4.2,4.1,4.2,4.1,4.2c0-0.6,0-1.2,0-1.8c0,0,0,0,0,0c0.2,0,0.4,0,0.6,0
      c0,0,0-0.1,0-0.1c0-1.1,0-2.3,0-3.4c0-0.2,0-0.5,0-0.7c0,0,0-0.1-0.1-0.1c-0.2,0-0.4,0-0.5,0c0,0,0-0.1,0-0.1c0-0.1,0-0.3,0-0.4
      c0,0,0-0.1,0-0.1c-0.2,0-0.4,0-0.6,0c0,0.6,0,1.2,0,1.8c-0.2,0-0.4,0-0.6,0c0-0.8,0-1.6,0-2.4c-0.4,0-0.8,0-1.2,0
      c0,0.6,0,1.2,0,1.8c-0.2,0-0.4,0-0.6,0c0,0,0-0.1,0-0.1c0-0.7,0-1.5,0-2.2c0,0,0-0.1,0-0.1l0,0c0.1,0,0.2,0,0.2,0
      c0.4,0,0.9,0,1.3,0c0.1,0,0.2,0,0.3,0c0,0.2,0,0.4,0,0.6c0.4,0,0.8,0,1.2,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6
      c0.2,0,0.4,0,0.6,0c0,1.4,0,2.8,0,4.3c-0.1,0-0.2,0-0.3,0c-0.1,0-0.2,0-0.3,0c0,0.1,0,0.3,0,0.4c0,0.4,0,0.9,0,1.3
      c0,0.1,0,0.1-0.1,0.1C4.5,4.2,4.3,4.2,4.1,4.2L4.1,4.2z"/>
    <path id="XMLID_171_" fill="#FFFFFF" d="M4.1,4.2c0,0.1,0,0.1,0,0.2c0,0.5,0,0.9,0,1.4c0,0.1,0,0.2,0,0.3C2.1,6,0,6-2,6
      c0-0.6,0-1.2,0-1.8c-0.2,0-0.4,0-0.6,0c0-0.4,0-0.8,0-1.2C-2.4,3-2.2,3-2,3c0,0.4,0,0.8,0,1.2c0.2,0,0.4,0,0.6,0c0,0.4,0,0.8,0,1.2
      c1.6,0,3.2,0,4.9,0c0-0.4,0-0.8,0-1.2C3.7,4.2,3.9,4.2,4.1,4.2L4.1,4.2z"/>
    <path id="XMLID_170_" fill="#FFFFFF" d="M-2-6.8c0,0.6,0,1.2,0,1.8c0,1.6,0,3.2,0,4.8c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0
      c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6l0,0c0.1,0,0.1,0,0.2,0c0.1,0,0.3,0,0.4,0c0-1.8,0-3.6,0-5.5
      c0,0,0.1,0,0.1,0C-2.4-6.8-2.2-6.8-2-6.8C-2.1-6.8-2-6.8-2-6.8L-2-6.8z"/>
    <path id="XMLID_169_" fill="#FFFFFF" d="M1.1-3.7C1-3.7,1-3.7,1.1-3.7c-0.4,0-0.8,0-1.2,0c0,0,0,0-0.1,0c0,0.8,0,1.6,0,2.4
      c-0.2,0-0.4,0-0.6,0c0-1.8,0-3.6,0-5.5c0.2,0,0.4,0,0.6,0c0,0.8,0,1.6,0,2.4c0.4,0,0.8,0,1.2,0c0,0,0,0.1,0,0.1C1-4.1,1-4,1.1-3.7
      C1-3.8,1-3.8,1.1-3.7L1.1-3.7z"/>
    <path id="XMLID_168_" fill="#FFFFFF" d="M-3.2,1.8c-0.1,0-0.2,0-0.3,0c-0.1,0-0.2,0-0.3,0c0,0-0.1,0-0.1-0.1c0-0.2,0-0.5,0-0.7
      c0-0.1,0-0.3,0-0.4c0,0,0,0,0-0.1c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0
      C-3.2,0.9-3.2,1.3-3.2,1.8c0.2,0,0.4,0,0.6,0c0,0.1,0,0.1,0,0.2c0,0.3,0,0.6,0,1C-2.6,3-2.7,3-2.7,3c-0.2,0-0.3,0-0.5,0
      C-3.2,2.6-3.2,2.2-3.2,1.8z"/>
    <path id="XMLID_167_" fill="#FFFFFF" d="M-3.2-1.3c-0.1,0-0.2,0-0.3,0c-0.3,0-0.5,0-0.8,0c0,0,0,0-0.1,0c0,0.4,0,0.8,0,1.2
      c-0.2,0-0.4,0-0.6,0c0-0.6,0-1.2,0-1.8c0.6,0,1.2,0,1.8,0c0,0.1,0,0.2,0,0.3C-3.2-1.5-3.2-1.4-3.2-1.3L-3.2-1.3z"/>
    <path id="XMLID_166_" fill="#FFFFFF" d="M-2-6.8C-2-7-2-7.2-2-7.4c0.4,0,0.8,0,1.2,0c0,0.2,0,0.4,0,0.6c-0.1,0-0.1,0-0.2,0
      C-1.3-6.8-1.6-6.8-2-6.8C-2-6.8-2-6.8-2-6.8L-2-6.8z"/>
  </g>
</svg>

<dt-article class="centered">
  $$
  \DeclareMathOperator*{\argmin}{{arg\,min}}
  \DeclareMathOperator*{\argmax}{{arg\,max}}
  \DeclareMathOperator*{\minimize}{{minimize}}
  $$


  <h1>Introduction to optimization algorithms</h1>
  <dt-byline class="l-page"></dt-byline>


  <h2>Mathematical Optimization</h2>
  <p><i>Mathematical Optimization is the branch of mathematics that aims to solve the problem of finding the elements that maximize or minimize a given function</i>. Its importance stems from the fact that many problems in engineering and machine learning can be cast as mathematical optimization problems. For example, in a spam detection filter we might aim to find the system that minimizes the number of misclassified emails. Similarly, when an engineer designs a pipe, we will seek for the design that minimizes cost while respecting some safety constraints. Both are examples that can be modeled as optimization problems.
  </p>

  <p><b>Note</b>: Optimization is a broad field. This lecture will be exclusively about <i>gradient-based</i> optimization methods. </p>

  <!-- <p>The birth of modern mathematical optimization is dated to the military services early in World War II. Other names for mathematical optimization mainly used during this initial period include "mathematical programming" and "operations research". Key figures of the initial period are Soviet economist Leonid Kantorovich<dt-cite key="kantorovich1940new"></dt-cite> and George Dantzig<dt-cite key="dantzig1951maximization"></dt-cite>. </p>

<figure>
  <img style="margin-left:50px;  width: 400px" src="http://worldwar2headquarters.com/images/normandy/landingCraft/armored-warfare.jpg" alt="">
 <figcaption id="expander" style="position:absolute; left:490px; top:30px; width:220px">Because of the war effort, there was an urgent need to allocate scarce resources
to different military operations in an effective manner. The British and then the U.S.
military management called upon a large number of scientists to apply some scientific approach for this strategic problems. The newly developed tools were
instrumental in winning the Air Battle of Britain and, later, played a major role in managing
antisubmarine operations for winning the Battle of the North Atlantic.
</figcaption>
</figure> -->


<h2>Notation</h2>
<p>Given an extended real-valued function $f: \RR^p \to \RR$, the general problem of finding the value that minimizes this function is written as follows
</p>
<p style="background-color: #D2E4FC; padding: 1px; border-radius: 8px;">\begin{equation}\label{eq:fw_objective}
  \minimize_{\boldsymbol{x} \in \RR^p} f(\boldsymbol{x}) ~,
\end{equation}</p>
<p>In this context, $f$ is the <i>objective function</i> (sometimes referred to as loss function, cost function or energy).</p>

<!-- <p>The beauty of optimization is that it allows to abstract from the details of the problem. Once posed within the optimization framework, we can use all its machinery to solve the problem without caring about where the problem came from. This has key advantages: we do not need to develop algorithms for each method separately, instead, the methods developed for a particular algorithm can be applied to any instance of of the same class of problems.</p> -->



  <h2>The rules of the game</h2>

  <p>Consider the following 2-dimensional optimization problem, with
$$
\minimize_{x \in \RR^2}\, \underbrace{(1 - x_1)^2 + 100(x_2 - x_1^2)^2}_{= f(x)}~.
$$
Since the <i>domain</i> of the objective function is a 2-dimensional space, we can visualize this objective function as an image in 2-D, where the color (darker=lower value) encodes the value of the objective function.
  </p>
  <figure style = "position:relative; width:984px; height:300px;">
    <div id="banana" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
  </figure>
  <p>A naive approach is to evaluate the function at a discrete grid and then just select the value that minimize the cost function. However, this approach is very inefficient: the number of function evaluation it requires is exponential in the number of dimensions.
  </p>

  <p>Let $n$ be the grid size in each dimension. Then this approach requires
$
n^d
$
function evaluations. Since we aim to solve problems with large $d$ (in the thousands if not millions), we need more efficient methods.</p>

  <p>A more efficient approach instead is to start from an initial guess and iteratively refine the initial guess. The information that to which we have access is the objective function value and its gradient
  </p>
  <figure style = "position:relative; width:984px; height:300px;">
    <div id="banana2" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
  </figure>
  <!-- <p>
  Like a hiker on a foggy day, we seek to reach the bottom of the valley while relying only on local information such as the value and steep (gradient) of the objective function at the current iterate.
  </p> -->
<br/>
<br/>
  <h1>Knowing your problem</h1>
  <dt-byline class="l-page"></dt-byline>


<p>Not all optimization problems are equal. Knowing your problem enables you
to choose the right tool.</p>
<div class="topic">
<h3>Dimensionality of the problem</h3>
<p>The scale of an optimization problem is pretty much set by the
<em>dimensionality of the problem</em>, i.e. the number of scalar variables
on which the search is performed.</p>
</div>
<div class="section" id="convex-versus-non-convex-optimization">
<h3>Convex versus non-convex optimization</h3>
<table class="docutils" border="1">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><img alt="convex_1d_1" src="https://scipy-lectures.org/_images/sphx_glr_plot_convex_001.png"></td>
<td><img alt="convex_1d_2" src="https://scipy-lectures.org/_images/sphx_glr_plot_convex_002.png"></td>
</tr>
<tr class="row-even"><td><p class="first"><strong>A convex function</strong>:</p>
<ul class="last simple">
<li><cite>f</cite> is above all its tangents.</li>
<li>equivalently, for two point A, B, f(C) lies below the segment
[f(A), f(B])], if A &lt; C &lt; B</li>
</ul>
</td>
<td><strong>A non-convex function</strong></td>
</tr>
</tbody>
</table>
<p><strong>Optimizing convex functions is typically easier than optimizing non-convex functions.</strong> Convex functions have the nice property that the gradient vanishes only at a global optimum</p>
</div>
<div class="section" id="smooth-and-non-smooth-problems">
<h3>Smooth and non-smooth problems</h3>
<table class="docutils" border="1">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><img alt="smooth_1d_1" src="https://scipy-lectures.org/_images/sphx_glr_plot_smooth_001.png"></td>
<td><img alt="smooth_1d_2" src="https://scipy-lectures.org/_images/sphx_glr_plot_smooth_002.png"></td>
</tr>
<tr class="row-even"><td><p class="first"><strong>A smooth function</strong>:</p>
<p class="last">The gradient is defined everywhere, and is a continuous function</p>
</td>
<td><strong>A non-smooth function</strong></td>
</tr>
</tbody>
</table>
<p><strong>Optimizing smooth functions is easier</strong>
(true in the context of general function, otherwise
<a class="reference external" href="http://en.wikipedia.org/wiki/Linear_programming">Linear Programming</a>
is an example of methods which deal very efficiently with
piece-wise linear functions).</p>
</div>
<div class="section" id="noisy-versus-exact-cost-functions">
<h3>Noisy versus exact cost functions</h3>
<table class="docutils" border="1">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Noisy (blue) and non-noisy (green) functions</td>
<td><img alt="noisy" src="https://scipy-lectures.org/_images/sphx_glr_plot_noisy_001.png"></td>
</tr>
</tbody>
</table>
<div class="topic">
<p>In some cases, computing the full objective and gradient is too costly, and we only have access to a <i>noisy</i> estimate of these quantities.</p>
</div>
</div>
<div class="section" id="constraints">
<h3>Constraints</h3>
<table class="docutils" border="1">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><p class="first">Optimizations under constraints</p>
<p>Here:</p>
<p><img class="math" src="https://scipy-lectures.org/_images/math/b270dcff9524408f385d632b77523aed51e9f444.png" alt="-1 < x_1 < 1"></p>
<p class="last"><img class="math" src="https://scipy-lectures.org/_images/math/b109b5cb82fcb53f0a1c2d3855cc5340c4598139.png" alt="-1 < x_2 < 1"></p>
</td>
<td><a class="reference external" href="auto_examples/plot_constraints.html"><img alt="constraints" src="https://scipy-lectures.org/_images/sphx_glr_plot_constraints_001.png"></a></td>
</tr>
</tbody>
</table>
</div>

<div class="section" id="condition">
<h3>Conditioning</h3>
<p>
  The speed of convergence of most optimization depend on the <i>conditioning</i> of our problem. A well conditioned problem is one in which the curvature is similar among any direction. A badly conditioned problem is one in which the level curves
</p>
<table class="docutils" border="1">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>
  Well conditioned problem
  <img src="assets/images/levelsets1.png" alt="">
</td>
<td>
  Badly conditioned problem
  <img src="assets/images/levelsets2.png" alt="">
</td>
</tr>
</tbody>
</table>
</div>

<h2>Recommended Readings</h2>
<p>
At the undergrade level I recommend the book Convex Optimization Algorithms by Dimitry Bertsekas<dt-cite key="bertsekas2015convex"></dt-cite>.
</p>
<p>At a more advanced level, the book by Nesterov<dt-cite key="nesterov2013introductory"></dt-cite> is also an excellent resource.</p>

<p>See also the great <a href="https://distill.pub/2017/momentum/">Distill article on momentum</a> from which much of this lecture was heavily inspired. </p>



<h2>Next: <a href="surrogate.html">Surrogate Optimization</a></h2>
<h2>Previous: <a href="index.html">Table of contents</a></h2>
<script src="assets/lib/contour_plot.js"></script>
<script src="assets/iterates.js"></script>
<script>

// Render Foreground
var iterControl = genIterDiagram(bananaf, [1,1/3], [[-2,2],[2/3 + 0.4,-2/3 + 0.4]], null, false, false, 0)
                  .alpha(0.003)
                  .beta(0)
                  (d3.select("#banana").style("position","relative"))


var iterControl = genIterDiagram(bananaf, [1,1/3], [[-2,2],[2/3 + 0.4,-2/3 + 0.4]], runGradientDescent, true, true, 300)
                  .alpha(0.003)
                  .beta(0)
                  (d3.select("#banana2").style("position","relative"))

// var iterChange = iterControl.control
// var getw0 = iterControl.w0
//
// var StepRange = d3.scaleLinear().domain([0,100]).range([0,1.00])
// var MomentumRange = d3.scaleLinear().domain([0,100]).range([0,0.98])
//
// var update = function (i,j) { iterChange(i, 0, getw0()) }
//
// var slidera = sliderGen([230, 40])
//             .ticks([0,0.02, 0.04, 1.0])
//             .ticktitles( function(d,i) { return ["0", "0.02", "0.04", "1.0"][i]})
//             .change( function (i) {
//               d3.select("#sliderAlpha").selectAll(".figtext").html("Step-size α = " + getalpha().toPrecision(2) )
//               iterChange(getalpha(), getbeta(), getw0() )
//             } )
//             .startxval(0.2)
//             .cRadius(7)
//             .shifty(-12)
//             .margins(20,20)
// var getalpha = slidera( d3.select("#sliderAlpha")).xval
// var getbeta = function() {return 0;}
// var getbeta  = sliderb( d3.select("#sliderBeta")).xval
//
// iterChange(getalpha(), getbeta(), getw0() )
</script>

</dt-article>

<dt-appendix class="centered">
</dt-appendix>

<script type="text/bibliography">
@article{kantorovich1940new,
  title={A new method of solving of some classes of extremal problems},
  author={Kantorovich, Leonid Vitalievich},
  journal={Dokl. Akad. Nauk SSSR},
  year={1940}
}
@book{calafiore2014optimization,
  title={Optimization models},
  author={Calafiore, Giuseppe C and El Ghaoui, Laurent},
  year={2014},
  publisher={Cambridge university press},
}
@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@book{bertsekas2015convex,
  title={Convex optimization algorithms},
  author={Bertsekas, Dimitri P and Scientific, Athena},
  year={2015},
  publisher={Athena Scientific Belmont}
}
@article{dantzig1951maximization,
  title={Maximization of a linear function of variables subject to linear inequalities},
  author={Dantzig, George B},
  journal={New York},
  year={1951}
}
</script>

<!-- Figure render queue -->
<script>

// for (var i = 0; i < renderQueue.length; i++) {
//   renderQueue[i](function() {})
//   deleteQueue[i](function() {})
// }
  setTimeout(function() {
    var q = d3.queue(1);

    d3.zip(deleteQueue,renderQueue).forEach(function(fn) {
      q.defer(function(callback) {
        fn[1](callback);
        fn[0](callback);
        renderMath(document.body);
      });
      q.defer(function(callback) {
        setTimeout(function() {
          callback(null);
        }, 50);
      });
    });
    q.await(function(error) {
      if (error) {
        console.error("Render error.", error)
      } else {
        console.log("Render done.")
      }
    });
  }, 50);

// DEBUG
// renderQueue.forEach( function (fn) { fn( function() {}) } )


</script>
